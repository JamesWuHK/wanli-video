#!/usr/bin/env python3
"""
ä½¿ç”¨ VectorEngine (sora-2) çš„æ··åˆæ–¹æ¡ˆåŠ¨æ€è§†é¢‘ç”Ÿæˆå™¨
- å…³é”®åˆ†é•œï¼šä½¿ç”¨ sora-2 AI ç”Ÿæˆï¼ˆÂ¥0.030/æ¬¡ï¼Œæœ€ä¾¿å®œï¼‰
- æ™®é€šåˆ†é•œï¼šä½¿ç”¨å¢å¼ºç‰ˆ Ken Burns æ•ˆæœ
"""

import os
import yaml
import subprocess
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
import edge_tts
from vectorengine_client import VectorEngineClient


class DynamicVideoGeneratorVE:
    """ä½¿ç”¨ VectorEngine çš„æ··åˆæ–¹æ¡ˆåŠ¨æ€è§†é¢‘ç”Ÿæˆå™¨"""

    def __init__(
        self,
        script_path: str,
        image_dir: str,
        keyframe_dir: str,
        output_dir: str,
        api_key: str = None,
        model: str = "sora-2",
        use_ai: bool = True
    ):
        """åˆå§‹åŒ–

        Args:
            script_path: è„šæœ¬æ–‡ä»¶è·¯å¾„
            image_dir: èµ·å§‹å¸§å›¾ç‰‡ç›®å½•
            keyframe_dir: å…³é”®å¸§å›¾ç‰‡ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            api_key: VectorEngine APIå¯†é’¥
            model: AIæ¨¡å‹åç§° (sora-2, veo_3_1-fast, grok-video-3)
            use_ai: æ˜¯å¦å¯ç”¨AIç”Ÿæˆ
        """
        self.script_path = script_path
        self.image_dir = Path(image_dir)
        self.keyframe_dir = Path(keyframe_dir)
        self.output_dir = Path(output_dir)
        self.model = model
        self.use_ai = use_ai

        # åˆ›å»ºå­ç›®å½•
        self.video_dir = self.output_dir / "videos"
        self.audio_dir = self.output_dir / "audio"
        self.temp_dir = self.output_dir / "temp"
        self.ai_cache_dir = self.output_dir / "ai_cache"

        for d in [self.video_dir, self.audio_dir, self.temp_dir, self.ai_cache_dir]:
            d.mkdir(parents=True, exist_ok=True)

        # åŠ è½½è„šæœ¬
        with open(script_path, 'r', encoding='utf-8') as f:
            self.script_data = yaml.safe_load(f)

        self.scenes = self.script_data.get('scenes', [])
        self.voice = self.script_data.get('project', {}).get('voice', 'zh-CN-YunxiNeural')

        # åˆå§‹åŒ– VectorEngine å®¢æˆ·ç«¯
        if self.use_ai:
            try:
                self.ve_client = VectorEngineClient(api_key=api_key)
                print(f"âœ… VectorEngine å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                print(f"   æ¨¡å‹: {self.model}")
            except Exception as e:
                print(f"âš ï¸  VectorEngine åˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"   å°†å›é€€åˆ°çº¯æœ¬åœ°å¤„ç†")
                self.use_ai = False

        print(f"âœ… åŠ è½½äº† {len(self.scenes)} ä¸ªåœºæ™¯")
        print(f"ğŸ™ï¸  ä½¿ç”¨è¯­éŸ³: {self.voice}")
        print(f"ğŸ¬ AI çŠ¶æ€: {'å¯ç”¨ (' + self.model + ')' if self.use_ai else 'ç¦ç”¨'}")

    def is_key_scene(self, scene: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®åˆ†é•œï¼ˆéœ€è¦ä½¿ç”¨AIç”Ÿæˆï¼‰

        åˆ¤æ–­æ ‡å‡†ï¼š
        1. åœºæ™¯æ ‡è®°ä¸º 'key': true
        2. æ—¶é•¿ >= 4ç§’
        3. åœºæ™¯æè¿°åŒ…å«åŠ¨ä½œè¯æ±‡
        """
        # æ˜¾å¼æ ‡è®°
        if scene.get('key', False):
            return True

        # æ—¶é•¿åˆ¤æ–­
        if scene.get('duration', 0) >= 4:
            return True

        # åŠ¨ä½œè¯æ±‡åˆ¤æ–­
        action_keywords = ['é£è¡Œ', 'ç§»åŠ¨', 'å¥”è·‘', 'è·³è·ƒ', 'æ—‹è½¬', 'é£˜åŠ¨', 'æµåŠ¨', 'ç”Ÿé•¿']
        description = scene.get('description', '') + scene.get('narration', '')

        for keyword in action_keywords:
            if keyword in description:
                return True

        return False

    async def generate_narration_audio(self, scene_id: str, narration: str) -> Path:
        """ç”Ÿæˆç”»å¤–éŸ³éŸ³é¢‘"""
        audio_path = self.audio_dir / f"{scene_id}.mp3"

        if audio_path.exists():
            return audio_path

        communicate = edge_tts.Communicate(narration, self.voice)
        await communicate.save(str(audio_path))

        return audio_path

    def get_audio_duration(self, audio_path: Path) -> float:
        """è·å–éŸ³é¢‘æ—¶é•¿"""
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            str(audio_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout.strip())

    def generate_ai_video(
        self,
        image_path: Path,
        prompt: str,
        duration: float,
        scene_id: str
    ) -> Optional[Path]:
        """ä½¿ç”¨ VectorEngine ç”ŸæˆåŠ¨æ€è§†é¢‘

        Args:
            image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
            prompt: è§†é¢‘ç”Ÿæˆæç¤ºè¯
            duration: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            scene_id: åœºæ™¯ID

        Returns:
            ç”Ÿæˆçš„è§†é¢‘è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.use_ai:
            return None

        output_video = self.ai_cache_dir / f"{scene_id}_{self.model}.mp4"

        # æ£€æŸ¥ç¼“å­˜
        if output_video.exists():
            print(f"   â­ï¸  AIè§†é¢‘å·²å­˜åœ¨ï¼Œä½¿ç”¨ç¼“å­˜")
            return output_video

        # è°ƒç”¨ VectorEngine API
        result = self.ve_client.generate_video_from_image(
            image_path=image_path,
            prompt=prompt,
            model=self.model,
            duration=int(duration),
            output_path=output_video
        )

        return result

    def create_ken_burns_video(
        self,
        image_path: Path,
        duration: float,
        output_path: Path,
        effect: str = "zoom_in"
    ):
        """åˆ›å»ºå¢å¼ºç‰ˆKen Burnsæ•ˆæœè§†é¢‘ï¼ˆç¼©æ”¾+å¹³ç§»+ç¼“åŠ¨æ›²çº¿ï¼‰"""

        # ï¿½ï¿½ä¹‰ä¸åŒçš„å¢å¼ºç‰ˆKen Burnsæ•ˆæœ
        # ä½¿ç”¨æ ‡å‡†æ•°å­¦å‡½æ•°å®ç°ç¼“åŠ¨æ›²çº¿ï¼Œè®©åŠ¨ç”»æ›´è‡ªç„¶ï¼Œæ¨¡æ‹Ÿç”µå½±æ„Ÿé•œå¤´è¿åŠ¨
        effects = {
            "zoom_in": {
                # ç¼“æ…¢æ¨è¿› - ä½¿ç”¨äºŒæ¬¡ç¼“åŠ¨æ›²çº¿ (easeInOut)
                # t = on/duration ä» 0 åˆ° 1ï¼Œä½¿ç”¨ t^2 è®©å¼€å§‹å’Œç»“æŸéƒ½å˜æ…¢
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.0+0.4*pow(on/{duration},2)':d={frames}:s=2048x1152:fps=30:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'",
            },
            "zoom_out": {
                # ç¼“æ…¢æ‹‰è¿œ - ä½¿ç”¨åå‘äºŒæ¬¡æ›²çº¿
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.4-0.4*pow(on/{duration},2)':d={frames}:s=2048x1152:fps=30:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'",
            },
            "pan_right": {
                # å³ç§» + è½»å¾®ç¼©æ”¾ - ä½¿ç”¨æ­£å¼¦æ³¢å¢åŠ åŠ¨æ„Ÿ
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.1+0.05*sin(on/{duration}*PI)':d={frames}:s=2048x1152:fps=30:x='(iw/2-(iw/zoom/2))+200*pow(on/{duration},1.5)':y='ih/2-(ih/zoom/2)'",
            },
            "pan_left": {
                # å·¦ç§» + è½»å¾®ç¼©æ”¾
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.1+0.05*sin(on/{duration}*PI)':d={frames}:s=2048x1152:fps=30:x='(iw/2-(iw/zoom/2))-200*pow(on/{duration},1.5)':y='ih/2-(ih/zoom/2)'",
            },
            "pan_up": {
                # ä¸Šç§» + ç¼©æ”¾ - è¥é€ å‡è…¾æ„Ÿ
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.0+0.3*pow(on/{duration},1.2)':d={frames}:s=2048x1152:fps=30:x='iw/2-(iw/zoom/2)':y='(ih/2-(ih/zoom/2))-150*pow(on/{duration},1.5)'",
            },
            "diagonal_in": {
                # å¯¹è§’çº¿æ¨è¿›ï¼ˆå³ä¸‹åˆ°å·¦ä¸Šï¼‰- ç»å…¸ç”µå½±é•œå¤´
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.0+0.35*pow(on/{duration},2)':d={frames}:s=2048x1152:fps=30:x='(iw/2-(iw/zoom/2))-120*pow(on/{duration},1.5)':y='(ih/2-(ih/zoom/2))-80*pow(on/{duration},1.5)'",
            },
            "circular": {
                # åœ†å‘¨è¿åŠ¨ + ç¼©æ”¾ - å¢åŠ åŠ¨æ€æ„Ÿ
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.15+0.05*sin(on/{duration}*2*PI)':d={frames}:s=2048x1152:fps=30:x='(iw/2-(iw/zoom/2))+100*sin(on/{duration}*2*PI)':y='(ih/2-(ih/zoom/2))+60*cos(on/{duration}*2*PI)'",
            },
            "breathe": {
                # å‘¼å¸æ„Ÿç¼©æ”¾ï¼ˆæ…¢-å¿«-æ…¢ï¼‰- ç”¨æ­£å¼¦æ³¢å®ç°è‡ªç„¶èŠ‚å¥
                "scale": "scale='if(eq(iw/ih,16/9),iw,ih*16/9)':'if(eq(iw/ih,16/9),ih,iw*9/16)',zoompan=z='1.0+0.15*sin(on/{duration}*PI)':d={frames}:s=2048x1152:fps=30:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'",
            }
        }

        frames = int(duration * 30)
        effect_config = effects.get(effect, effects["zoom_in"])
        filter_str = effect_config["scale"].format(frames=frames, duration=duration)

        cmd = [
            'ffmpeg', '-y',
            '-loop', '1',
            '-i', str(image_path),
            '-vf', filter_str,
            '-t', str(duration),
            '-c:v', 'libx264',
            '-preset', 'slow',  # æ›´å¥½çš„å‹ç¼©è´¨é‡
            '-crf', '18',  # é«˜è´¨é‡ï¼ˆ0-51ï¼Œè¶Šå°è¶Šå¥½ï¼‰
            '-pix_fmt', 'yuv420p',
            str(output_path)
        ]

        subprocess.run(cmd, check=True, capture_output=True)

    def add_subtitle_to_video(
        self,
        video_path: Path,
        subtitle_text: str,
        output_path: Path
    ):
        """ä¸ºè§†é¢‘æ·»åŠ å­—å¹•"""

        # æ ¹æ®ç³»ç»Ÿé€‰æ‹©å­—ä½“æ–‡ä»¶
        import platform
        if platform.system() == 'Darwin':  # macOS
            fontfile = '/System/Library/Fonts/PingFang.ttc'
        else:  # Linux (Docker)
            fontfile = '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc'

        fontsize = 32
        fontcolor = 'white'
        box = 1
        boxcolor = 'black@0.5'

        subtitle_escaped = subtitle_text.replace("'", "'\\''").replace(":", "\\:")

        filter_str = (
            f"drawtext=text='{subtitle_escaped}':"
            f"fontfile='{fontfile}':"
            f"fontsize={fontsize}:"
            f"fontcolor={fontcolor}:"
            f"box={box}:"
            f"boxcolor={boxcolor}:"
            f"x=(w-text_w)/2:"
            f"y=h-100"
        )

        cmd = [
            'ffmpeg', '-y',
            '-i', str(video_path),
            '-vf', filter_str,
            '-c:v', 'libx264',
            '-pix_fmt', 'yuv420p',
            '-c:a', 'copy' if video_path.suffix == '.mp4' else 'aac',
            str(output_path)
        ]

        subprocess.run(cmd, check=True, capture_output=True)

    def merge_video_audio(
        self,
        video_path: Path,
        audio_path: Path,
        output_path: Path
    ):
        """åˆå¹¶è§†é¢‘å’ŒéŸ³é¢‘"""
        cmd = [
            'ffmpeg', '-y',
            '-i', str(video_path),
            '-i', str(audio_path),
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-shortest',
            str(output_path)
        ]
        subprocess.run(cmd, check=True, capture_output=True)

    async def create_scene_video(self, scene_index: int):
        """ä¸ºå•ä¸ªåœºæ™¯åˆ›å»ºåŠ¨æ€è§†é¢‘"""
        scene = self.scenes[scene_index]
        scene_id = scene['id']
        duration = scene['duration']
        narration = scene.get('narration', '')
        image_prompt = scene.get('image_generation_prompt', '')

        print(f"\nğŸ¬ åœºæ™¯ {scene_index + 1}/{len(self.scenes)}: {scene_id}")
        print(f"   â±ï¸  æ—¶é•¿: {duration}ç§’")
        print(f"   ğŸ“ æç¤ºè¯: {image_prompt[:80]}...")

        # åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®åˆ†é•œ
        is_key = self.is_key_scene(scene)
        method = f'AI ({self.model})' if is_key and self.use_ai else 'Ken Burns'
        print(f"   {'ğŸ”‘ å…³é”®åˆ†é•œ' if is_key else 'ğŸ“¹ æ™®é€šåˆ†é•œ'} - ä½¿ç”¨ {method}")

        # 1. ç”Ÿæˆç”»å¤–éŸ³
        audio_path = await self.generate_narration_audio(scene_id, narration)
        audio_duration = self.get_audio_duration(audio_path)
        actual_duration = max(duration, audio_duration + 0.5)

        # è·å–å›¾ç‰‡ï¼ˆä¼˜å…ˆä½¿ç”¨ doubao_images ä¸­çš„è®¾è®¡å›¾ï¼ŒåŒ…å«æ­£ç¡®çš„ä¸­æ–‡å­—ä½“ï¼‰
        image_path = self.image_dir / f"{scene_id}.png"
        if not image_path.exists():
            # å›é€€åˆ° keyframes
            image_path = self.keyframe_dir / f"{scene_id}_keyframe.png"
            if not image_path.exists():
                raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")

        # 2. ç”Ÿæˆè§†é¢‘ï¼ˆAI æˆ– Ken Burnsï¼‰
        video_no_subtitle = self.temp_dir / f"{scene_id}_no_subtitle.mp4"

        if is_key and self.use_ai:
            # ä½¿ç”¨ AI ç”Ÿæˆ
            ai_prompt = image_prompt if image_prompt else narration
            ai_video = self.generate_ai_video(
                image_path,
                ai_prompt,
                actual_duration,
                scene_id
            )

            if ai_video and ai_video.exists():
                # æˆåŠŸä½¿ç”¨ AI
                video_no_subtitle = ai_video
            else:
                # AI å¤±è´¥ï¼Œå›é€€åˆ° Ken Burns
                print(f"   âš ï¸  AIç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ° Ken Burns æ•ˆæœ")
                self.create_ken_burns_video(
                    image_path,
                    actual_duration,
                    video_no_subtitle,
                    effect="zoom_in"
                )
        else:
            # ä½¿ç”¨å¢å¼ºç‰ˆ Ken Burns æ•ˆæœ
            # æ ¹æ®åœºæ™¯å†…å®¹æ™ºèƒ½é€‰æ‹©æ•ˆæœ
            effects = [
                "zoom_in",      # æ¨è¿›
                "zoom_out",     # æ‹‰è¿œ
                "pan_right",    # å³ç§»
                "pan_left",     # å·¦ç§»
                "pan_up",       # ä¸Šç§»
                "diagonal_in",  # å¯¹è§’çº¿
                "circular",     # åœ†å‘¨
                "breathe"       # å‘¼å¸
            ]

            # æ ¹æ®åœºæ™¯åºå·å’Œæè¿°é€‰æ‹©åˆé€‚çš„æ•ˆæœ
            if 'opening' in scene_id or 'grand' in scene_id:
                effect = "zoom_in"  # å¼€åœºå’Œç»“å°¾ç”¨æ¨è¿›
            elif 'intro' in scene_id or 'history' in scene_id:
                effect = "zoom_out"  # ä»‹ç»æ€§åœºæ™¯ç”¨æ‹‰è¿œ
            elif 'modern' in scene_id:
                effect = "pan_right"  # ç°ä»£åœºæ™¯ç”¨å¹³ç§»
            else:
                effect = effects[scene_index % len(effects)]

            print(f"   ğŸ¨ åº”ç”¨å¢å¼ºç‰ˆ Ken Burns æ•ˆæœ: {effect}")
            self.create_ken_burns_video(
                image_path,
                actual_duration,
                video_no_subtitle,
                effect=effect
            )

        # 3. æ·»åŠ å­—å¹•
        video_with_subtitle = self.temp_dir / f"{scene_id}_subtitle.mp4"

        print(f"   ğŸ“ æ·»åŠ å­—å¹•...")
        self.add_subtitle_to_video(
            video_no_subtitle,
            narration,
            video_with_subtitle
        )

        # 4. åˆå¹¶éŸ³é¢‘
        output_video = self.video_dir / f"{scene_id}.mp4"

        print(f"   ğŸµ åˆå¹¶éŸ³é¢‘...")
        self.merge_video_audio(
            video_with_subtitle,
            audio_path,
            output_video
        )

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if video_no_subtitle != output_video and video_no_subtitle.parent == self.temp_dir:
            video_no_subtitle.unlink(missing_ok=True)
        video_with_subtitle.unlink(missing_ok=True)

        print(f"   âœ… å®Œæˆ: {output_video.name}")
        return output_video

    async def generate_all_videos(self, max_concurrent: int = 3):
        """ç”Ÿæˆæ‰€æœ‰åœºæ™¯è§†é¢‘ï¼ˆæ”¯æŒå¹¶å‘ï¼‰

        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ä¸ªï¼Œé¿å…APIé™æµï¼‰
        """
        print("=" * 70)
        print("ğŸ¬ VectorEngine æ··åˆæ–¹æ¡ˆåŠ¨æ€è§†é¢‘ç”Ÿæˆ (å¹¶å‘æ¨¡å¼)")
        print("=" * 70)

        key_scenes_count = sum(1 for s in self.scenes if self.is_key_scene(s))

        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   æ€»åœºæ™¯æ•°: {len(self.scenes)}")
        print(f"   å…³é”®åˆ†é•œ: {key_scenes_count} (ä½¿ç”¨ {self.model})")
        print(f"   æ™®é€šåˆ†é•œ: {len(self.scenes) - key_scenes_count} (ä½¿ç”¨ Ken Burns)")
        print(f"   ğŸš€ å¹¶å‘æ•°: {max_concurrent} ä¸ªåœºæ™¯åŒæ—¶ç”Ÿæˆ")
        if self.use_ai:
            estimated_cost = key_scenes_count * 0.078  # veo_3_1-fast ä»·æ ¼
            print(f"   é¢„ä¼°æˆæœ¬: Â¥{estimated_cost:.2f}")
        print()

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def create_with_semaphore(index):
            async with semaphore:
                try:
                    return await self.create_scene_video(index), None
                except Exception as e:
                    scene_id = self.scenes[index]['id']
                    print(f"   âŒ {scene_id} å¤±è´¥: {str(e)}")
                    return None, (scene_id, str(e))

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åœºæ™¯
        results = await asyncio.gather(*[create_with_semaphore(i) for i in range(len(self.scenes))])

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r, e in results if r is not None)
        failed_scenes = [e for r, e in results if e is not None]

        print("\n" + "=" * 70)
        print(f"âœ… å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{len(self.scenes)} ä¸ªè§†é¢‘")

        if failed_scenes:
            print(f"\nâš ï¸  å¤±è´¥åœºæ™¯ï¼š")
            for sid, error in failed_scenes:
                print(f"   - {sid}: {error[:60]}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.video_dir}")
        print("=" * 70)


async def main():
    """ä¸»å‡½æ•°"""

    # é…ç½®
    API_KEY = os.getenv('VECTORENGINE_API_KEY')  # ä»ç¯å¢ƒå˜é‡è¯»å–
    MODEL = os.getenv('AI_MODEL', 'sora-2')  # é»˜è®¤ä½¿ç”¨ sora-2
    USE_AI = os.getenv('USE_AI', 'true').lower() == 'true'

    if not API_KEY and USE_AI:
        print("âš ï¸  æœªè®¾ç½® VECTORENGINE_API_KEY ç¯å¢ƒå˜é‡")
        print("   å°†ç¦ç”¨AIåŠŸèƒ½ï¼Œä»…ä½¿ç”¨Ken Burnsæ•ˆæœ")
        print("   å¦‚éœ€å¯ç”¨AIï¼Œè¯·è®¾ç½®:")
        print("   export VECTORENGINE_API_KEY='your-api-key'")
        print()
        USE_AI = False

    generator = DynamicVideoGeneratorVE(
        script_path='./æ–‡è„‰è–ªä¼ _ç»†åŒ–è„šæœ¬.yaml',
        image_dir='./storyboards/æ–‡è„‰è–ªä¼ /doubao_images',
        keyframe_dir='./storyboards/æ–‡è„‰è–ªä¼ /keyframes',
        output_dir='./storyboards/æ–‡è„‰è–ªä¼ /dynamic_videos_ve',
        api_key=API_KEY,
        model=MODEL,
        use_ai=USE_AI
    )

    await generator.generate_all_videos()


if __name__ == "__main__":
    asyncio.run(main())
